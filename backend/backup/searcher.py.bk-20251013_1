import requests
import feedparser
from scholarly import scholarly
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from datetime import datetime, timedelta
import logging
import time
import urllib.parse
from bs4 import BeautifulSoup

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

G6_KEYWORDS = [
    '6G wireless communication',
    '6G terahertz communication',
    '6G ultra-massive MIMO',
    '6G integrated sensing and communication',
    '6G quantum communication',
    '6G dynamic spectrum sharing',
    '6G AI-native networks',
    '6G holographic connectivity',
    '6G ubiquitous connectivity',
    '6G deep connectivity',
    '6G intelligent connectivity',
    '6G THz bands',
    '6G in-band full-duplex',
    '6G visible light communication',
    '6G orbital angular momentum',
    '6G energy efficiency',
    '6G ultra-reliable low latency',
    '6G ultra-high reliability',
    '6G spectral efficiency',
    '6G machine learning',
    '6G edge computing',
    '6G quantum key distribution',
    '6G backscatter communications',
    '6G multiuser MIMO',
    '6G post-quantum security'
]

headers = {
    'User-Agent': '6G-Articles-App/1.0 (mailto:amir.gr86@gmail.com)',
    'Accept': 'application/json'
}

'''def arxiv_search(query='6G', max_results=5):
    base = 'http://export.arxiv.org/api/query?'
    # Use broader query for more results
    broad_query = '6G OR ' + urllib.parse.quote(query)
    params = f'search_query=all:{broad_query}&start=0&max_results={max_results}&sortBy=submittedDate&sortOrder=descending'
    try:
        response = requests.get(base + params, timeout=10, headers=headers)
        response.raise_for_status()
        feed = feedparser.parse(response.text)
        articles = []
        for entry in feed.entries:
            authors = ', '.join(author.name for author in entry.authors)
            if len(authors) > 1000:
                authors = authors[:950] + ' ... et al.'
            articles.append({
                'title': entry.title,
                'authors': authors,
                'publish_date': datetime.strptime(entry.published[:10], '%Y-%m-%d').date(),
                'link': entry.link,
                'full_text': entry.summary
            })
        logger.info(f"arXiv fetched {len(articles)} articles for query '{query}'")
        return articles
    except Exception as e:
        logger.error(f"arXiv error for query '{query}': {e}")
        return []
'''
def arxiv_search(query='6G', max_results=10):  # Increased to 10
    base = 'http://export.arxiv.org/api/query?'
    params = {
        'search_query': urllib.parse.quote(query),
        'start': '0',
        'max_results': str(max_results),
        'sortBy': 'relevance',  # Changed from submittedDate to relevance
        'sortOrder': 'descending'
    }
    url = base + urllib.parse.urlencode(params)
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        logger.info(f"arXiv URL: {url}")
        logger.info(f"arXiv raw response (first 1000 chars): {response.text[:1000]}")
        if '<opensearch:totalResults>' in response.text:
            total = int(BeautifulSoup(response.text, 'xml').find('opensearch:totalresults').text)
            logger.info(f"arXiv total results: {total}")
        feed = feedparser.parse(response.text)
        articles = []
        for entry in feed.entries:
            authors = ', '.join(author.name for author in entry.authors)
            if len(authors) > 1000:
                authors = authors[:950] + ' ... et al.'
            articles.append({
                'title': entry.title,
                'authors': authors,
                'publish_date': datetime.strptime(entry.published[:10], '%Y-%m-%d').date(),
                'link': entry.link,
                'full_text': entry.summary
            })
        logger.info(f"arXiv fetched {len(articles)} articles for query '{query}'")
        return articles
    except Exception as e:
        logger.error(f"arXiv error for query '{query}': {e}")
        return []

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10), retry=retry_if_exception_type(requests.exceptions.HTTPError))
def semantic_search(query='6G wireless communication', max_results=5):
    url = 'https://api.semanticscholar.org/graph/v1/paper/search'
    params = {'query': urllib.parse.quote(query), 'limit': max_results, 'fields': 'title,authors,publicationDate,url,abstract'}
    try:
        response = requests.get(url, params=params, timeout=10, headers=headers)
        if response.status_code == 429:
            logger.warning(f"Semantic Scholar rate limit (429) for query '{query}', waiting...")
            time.sleep(30)  # Longer wait for 429
            return []
        response.raise_for_status()
        data = response.json()
        articles = []
        for p in data.get('data', []):
            if p.get('abstract') and len(p['abstract']) > 50:
                authors = ', '.join(a['name'] for a in p.get('authors', []))
                if len(authors) > 1000:
                    authors = authors[:950] + ' ... et al.'
                articles.append({
                    'title': p['title'],
                    'authors': authors,
                    'publish_date': datetime.strptime(p['publicationDate'], '%Y-%m-%d').date() if p.get('publicationDate') else None,
                    'link': p['url'],
                    'full_text': p['abstract']
                })
        logger.info(f"Semantic Scholar fetched {len(articles)} articles for query '{query}'")
        return articles
    except requests.exceptions.HTTPError as e:
        if response.status_code == 429:
            logger.warning(f"Semantic Scholar rate limit (429) for query '{query}', returning empty list")
            return []
        logger.error(f"Semantic Scholar error for query '{query}': {e}")
        raise
    except Exception as e:
        logger.error(f"Semantic Scholar error for query '{query}': {e}")
        return []

def openalex_search(query='6G wireless communication', max_results=5):
    logger.info(f"Skipping OpenAlex for query '{query}' due to persistent 403 errors")
    return []

def scholarly_search(query='6G wireless communication', max_results=5):
    try:
        search_query = scholarly.search_pubs(query)
        articles = []
        for i, result in enumerate(search_query):
            if i >= max_results:
                break
            authors = ', '.join(result['bib']['author'] or [])
            if len(authors) > 1000:
                authors = authors[:950] + ' ... et al.'
            articles.append({
                'title': result['bib']['title'],
                'authors': authors,
                'publish_date': datetime.strptime(result['bib']['pub_year'], '%Y').date() if result['bib'].get('pub_year') else None,
                'link': result.get('eprinturl', result['pub_url']),
                'full_text': result.get('abstract', '')
            })
            time.sleep(1)
        logger.info(f"Google Scholar fetched {len(articles)} articles for query '{query}'")
        return articles
    except Exception as e:
        logger.error(f"Google Scholar error for query '{query}': {e}")
        return []

def weekly_search():
    all_articles = []
    # Use all keywords for more results
    selected_keywords = G6_KEYWORDS  # All 24 keywords
    for keyword in selected_keywords:
        all_articles += arxiv_search(keyword)
        all_articles += semantic_search(keyword)
        # all_articles += openalex_search(keyword)  # Disabled
        time.sleep(5)  # Increased delay for rate limiting

    all_articles += scholarly_search(G6_KEYWORDS[0])  # Google Scholar for first keyword

    seen = set()
    unique = []
    #one_month_ago = datetime.now() - timedelta(days=90)
    for a in all_articles:
        if a.get('publish_date') and a['publish_date'] < one_month_ago.date():
            continue  # Skip articles older than 30 days
        key = (a['title'], a.get('link', ''))
        if key not in seen and len(unique) < 10:
            seen.add(key)
            unique.append(a)

    logger.info(f"Fetched {len(all_articles)} total articles, {len(unique)} unique after dedup (capped at 10)")
    return unique

def arxiv_web_search(query='6G', max_results=5):
    url = f'https://arxiv.org/search/?query={urllib.parse.quote(query)}&searchtype=all&abstracts=show&order=-announced_date_first&size=50'
    response = requests.get(url, headers=headers, timeout=10)
    soup = BeautifulSoup(response.text, 'html.parser')
    results = soup.find_all('li', class_='arxiv-result')
    articles = []
    for result in results[:max_results]:
        title = result.find('p', class_='title').text.strip() if result.find('p', class_='title') else ''
        authors = result.find('p', class_='authors').text.strip() if result.find('p', class_='authors') else ''
        link = 'https://arxiv.org' + result.find('a', class_='title')['href'] if result.find('a', class_='title') else ''
        articles.append({
            'title': title,
            'authors': authors,
            'publish_date': None,  # Scrape date if needed
            'link': link,
            'full_text': 'Summary not scraped'  # Would need to scrape abstract page
        })
    logger.info(f"arXiv web search fetched {len(articles)} articles for query '{query}'")
    return articles
